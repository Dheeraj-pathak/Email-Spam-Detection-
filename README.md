ðŸ“˜ SMS Spam Classification using LSTM â€“ README
ðŸ“Œ Project Overview

This project builds an SMS Spam Classifier using Deep Learning (LSTM).
The model reads short text messages and predicts whether they are:

Ham (0) â†’ Normal message

Spam (1) â†’ Unwanted promotional or fraudulent message
<<<<<<< HEAD
=======

It uses:

Python

TensorFlow/Keras

LSTM (Long Short-Term Memory)

Tokenization + Embedding

Confusion Matrix + Performance Metrics

This project demonstrates a complete workflow of natural language processing, sequence modeling, and deep learning classification.

ðŸ§  Why LSTM?

SMS messages are sequences of words, and meaning depends on order.
Example:

â€œFree ticket nowâ€ â†’ spam

â€œAre you free now?â€ â†’ ham

LSTM is perfect because it:

remembers long-term dependencies

captures sequence patterns

handles text context better than simple models

ðŸš€ Features

âœ” Complete cleaning of raw SMS text
âœ” Tokenization + Padding
âœ” LSTM-based sequential model
âœ” Bidirectional LSTM for better context
âœ” Training, validation, and testing pipeline
âœ” Confusion matrix and metrics
âœ” Saved model + tokenizer for future use

ðŸ—‚ï¸ Dataset

The project uses the popular SMS Spam Collection Dataset, which contains:

4825 ham messages

747 spam messages

Format:

label, message
ham, Go until jurong point...
spam, Congratulations you won...


If using Kaggle version:

Column names may appear as v1 (label), v2 (message).
The script automatically renames them.

ðŸ§¹ Text Preprocessing Pipeline

The script performs the following steps:

Convert to lowercase

Remove URLs

Remove email addresses

Remove numeric values

Remove punctuation

Clean whitespace

Tokenize using regex (no NLTK required)

Remove stopwords

Join tokens back into a clean sentence

This improves signal quality for the LSTM.

ðŸ”¢ Tokenization + Padding

Keras Tokenizer converts words â†’ integers.

Vocabulary size: 10,000 words

Max sequence length: 100 tokens

OOV token: <OOV> (handles unknown words)

Padding ensures fixed-length inputs for the network.

ðŸ§± Model Architecture

The LSTM model consists of:

Embedding Layer (100 dimensions)
Bidirectional LSTM (128 units)
Dropout (0.4)
Dense (64 units, ReLU)
Dropout (0.3)
Dense (1 unit, Sigmoid)

Loss & Optimizer

Loss: Binary Cross-Entropy

Optimizer: Adam

Metrics: Accuracy

ðŸ‹ï¸ Training Setup

Batch size: 64

Epochs: 10

Validation split: 10%

Callbacks:

EarlyStopping

ModelCheckpoint (saves best model automatically)

ðŸ“Š Evaluation Metrics

After training, the model generates:

Accuracy

Precision

Recall

F1-Score

Confusion Matrix

These help evaluate correctness on both ham and spam messages.
ðŸ Results (Typical Performance)

Most runs achieve:

Accuracy: 95%+

Precision (Spam): 90%+

Recall (Spam): 85%+

ðŸ“ Future Improvements

Use GRU or Transformer-based models

Add stemming/lemmatization

Add TF-IDF + classical ML models

Deploy using Flask/FastAPI

Create a web UI

ðŸ‘¤ Author

Dheeraj
AIML Student & Android Developer
>>>>>>> 7451d44 (Final commit)
